1. 客户端建立连接以后，你是如何处理读事件的？
使用连接获取的connect文件描述符，使用read系统调用，获取读取字节。
如果小于0 报错，如果为0，可能是对端已经关闭，按照已经关闭处理。
http请求分为几个不同的阶段：解析状态头，解析错误，解析post，解析body，解析数据。
然后调用写事件。
写事件只是往文件描述符中写outBuffer就可以了。


2. epoll的边缘触发和水平触发有什么区别？为什么选择这种实现？代码中如何体现的？
电平触发在有事件发生的时候，可以不立即处理该事件，边沿触发必须立刻处理该事件。
ET确保事件只触发一次，更快并且高效。
在代码中，体现在读取缓存的时候，ET必须循环while读取，确保把读缓存的事件全部读出。
此处注意，边沿触发只通知一次，并不会清空缓冲区，而水平触发只要缓冲区有数据就一直触发。


3. 说一下解析请求的流程
parseURI->parseHeaders->analysisRequest
开始先解析urI，去掉请求行，用字符串处理找到GET POST或者HEAD字段，设置相关的数值，
找到文件名，用substr设置相关字段，
同样找到版本号等
然后开始解析请求头，请求头部包括Connection、Content-Length、Host等字段
然后如果是post方法，要解析body

4. handleConn什么时候调用？
每一次有链接都调用，检查是否存活和连接超时。

5. 如何调试多线程程序？
对于代码，可以使用gdb调试，先运行，然后grep找到，attach附加。
tcpdump 抓包
lsof 列出系统当前打开的文件描述符，查看端口占用lsof -i
nc 快速发送数据
strace 跟踪过系统调用和接收到的信号
netstat 看完罗信息
vmstat 输出系统资源使用情况
ifstat 网络流量监测
mpstat 监测cpu使用情况

6. 你如何进行的测试？
为套接字设置nodelay关闭naggle算法
使用webbench进行测试，短链接达到10w+QPS，长连接达到30w+QPS
使用top进行了CPU负载测试
使用Valgrind进行内存泄露测试

7. 长连接和短链接分别如何处理？
只要不执行close，那么就能实现长连接

8. 如何提高服务器效率？（此处结合muduo库说一下）
改string为buffer，提高性能，在数据量小的时候，header解析换用map而非unordered_map，因为红黑树占用的内存比哈希表更小
buffer是一块连续的内存区域，分为input buffer和output buffer，用于读取socket和写入缓冲区

9. 如何处理文件访问请求的？这样有什么问题？如何改进？




10. 如果客户端连接数量超过线程，会发生什么？如何改进？
《linux多线程服务端编程》6.6.2介绍的方案11中介绍了这种情况，可以开一个线程池，把reactor中耗时的计算任务交给线程池。


11. 说一下线程池建立流程



12. 说一下项目loop流程，怎么启动起来的？各模块关系是什么？




13. 为什么采用多线程而非多进程？
多线程能够发挥多核并发优势，更容易进行通信，能够更好的实现模块逻辑优势，one thread one loop


14. 使用 Epoll 边沿触发的 IO 多路复用技术，非阻塞 IO，使用 Reactor 模式。那么什么是epoll？什么是IO多路复用？什么是Reactor模式？

Reactor 模式要求 主线程（I/O 处理单元） 只负责监听文件描述符上是否有事件发生，有的话就立即将该事件通知工作线程（逻辑单元）。除此之外，主线程不做任何其他实质性的工作。 读写数据，接受新的连接，以及处理客户请求均在工作线程中完成。
Proactor则全部IO都在主线程上完成

15. 使用基于小根堆的定时器关闭超时请求 怎么实现的？

16. 使用双缓冲区技术实现了简单的异步日志系统 如何实现的？

17. 使用状态机解析了 HTTP 请求,支持管线化 什么是管线化？

18. 你这个项目难点在哪里？如何解决的？



19. 如何测试的？

20. 你了解五种IO模型吗？
阻塞IO、非阻塞IO、IO多路复用、信号驱动IO、异步IO


21. 如何理解IO多路复用的复用？
复用指的是复用一个线程，在一个线程里面实现对多个io事件的监听和处理

22. epoll中用到的重要结构有哪些（说一下epoll调用的函数和过程）
1.epoll_create创建内核事件表，返回这个事件表的文件描述符
函数参数是内核描述符的大小
2.epoll_ctl传入文件描述出，epoll_event指定事件和用户数据，epoll_data_t指定用户数据（面腾讯问到了）
这个函数参数是文件描述符，操作类型，事件指针（epoll_event结构体指针）
3.epoll_wait在一段时间之内等待文件描述符上的事件
这个函数的参数 包括内核事件表文件描述符 要复制到的数组 最多监听的文件数量和过期时间

23. 说一下functor和std::fuction的使用场景？各自有什么好处？



24. 为什么要用lambda代替bind?
编译器往往不会内联函数指针，bind会被编译为函数指针，比lambda更慢
lambda更清晰易懂
bind在设置定时器的时候，会设置成bind调用的的时间而非定时器的时间
在c++11中，移动捕获和多态的情况用bind更合适


25. eventfd和管道的区别？为什么用eventfd？read、write和mmap有什么区别？

第一，是打开文件数量的巨大差别。由于pipe是半双工的传统IPC方式，所以两个线程通信需要两个pipe文件，而用eventfd只要打开一个文件。众所周知，文件描述符可是系统中非常宝贵的资源，linux的默认值也只有1024而已。那开发者可能会说，1相比2也只节省了一半嘛。要知道pipe只能在两个进程/线程间使用，并且是面向连接（类似TCP socket）的，即需要之前准备好两个pipe；而eventfd是广播式的通知，可以多对多的。如上面的NxM的生产者-消费者例子，如果需要完成全双工的通信，需要NxMx2个的pipe，而且需要提前建立并保持打开，作为通知信号实在太奢侈了，但如果用eventfd，只需要在发通知的时候瞬时创建、触发并关闭一个即可。
第二，是内存使用的差别。eventfd是一个计数器，内核维护几乎成本忽略不计，大概是自旋锁+唤醒队列（后续详细介绍），8个字节的传输成本也微乎其微。但pipe可就完全不是了，一来一回数据在用户空间和内核空间有多达4次的复制，而且更糟糕的是，内核还要为每个pipe分配至少4K的虚拟内存页，哪怕传输的数据长度为0。
第三，对于timerfd，还有精准度和实现复杂度的巨大差异。由内核管理的timerfd底层是内核中的hrtimer（高精度时钟定时器），可以精确至纳秒（1e-9秒）级，完全胜任实时任务。而用户态要想实现一个传统的定时器，通常是基于优先队列/二叉堆，不仅实现复杂维护成本高，而且运行时效率低，通常只能到达毫秒级。

mmap只需要一次系统调用，后续操作不需要系统调用
访问的数据不需要在page cache和用户缓冲区之间拷贝
从上所述，当频繁对一个文件进行读取操作时，mmap会比read高效一些。

26. read/write是怎么做的？
访问文件，这涉及到用户态到内核态的转换
读取硬盘文件中的对应数据，内核会采用预读的方式，比如我们需要访问100字节，内核实际会将按照4KB(内存页的大小)存储在page cache中
将read中需要的数据，从page cache中拷贝到用户缓冲区中

27. 说一下你的主从状态机，状态怎么转换的？
从状态机三种状态
LINE_OK ,完整读取一行
LINE_OPEN，读取的行不完整
LINE_BAD，读取的报文有误
从状态机每次从缓冲区读取一行信息，直至读取到 \r\n 表示读取到一行，同时将 \r\n 替换为 \0\0 便于主状态机读取该行，然后再将行起始标志定位到下一行的起始位置。
主状态机有三种状态
CHECK_STATE_REQUESTLINE，解析请求行
CHECK_STATE_HEADER，解析头部信息
CHECK_STATE_CONTENT，解析正文


28. send阻塞吗



